{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Inspecting the QAT GPT-2 Checkpoint\n",
    "\n",
    "This notebook shows how to properly load and inspect the saved .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Checkpoint File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Checkpoint file found at: qat_gpt2_8bit_fp32_20250917_113554.pth\n",
      "File size: 505.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Path to your checkpoint\n",
    "checkpoint_path = 'qat_gpt2_8bit_fp32_20250917_113554.pth'\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"✓ Checkpoint file found at: {checkpoint_path}\")\n",
    "    print(f\"File size: {os.path.getsize(checkpoint_path) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"✗ File not found at: {checkpoint_path}\")\n",
    "    print(\"Please update the path to your checkpoint file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "✓ Checkpoint loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "print(\"\\n✓ Checkpoint loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspect Checkpoint Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys in checkpoint:\n",
      "==================================================\n",
      "1. model_state_dict\n",
      "2. model_config\n",
      "3. training_config\n",
      "4. timestamp\n"
     ]
    }
   ],
   "source": [
    "# Show top-level keys in the checkpoint\n",
    "print(\"Top-level keys in checkpoint:\")\n",
    "print(\"=\"*50)\n",
    "for i, key in enumerate(checkpoint.keys(), 1):\n",
    "    print(f\"{i}. {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed checkpoint contents:\n",
      "==================================================\n",
      "\n",
      "'model_state_dict':\n",
      "  Type: dict with 1571 items\n",
      "  Sample keys: ['wte.weight', 'wpe.weight', 'h.0.ln_1.weight', 'h.0.ln_1.bias', 'h.0.attn.bias']\n",
      "\n",
      "'model_config':\n",
      "  Type: dict with 22 items\n",
      "\n",
      "'training_config':\n",
      "  Type: dict with 17 items\n",
      "    train_split: train[:5000]\n",
      "    val_split: validation[:1000]\n",
      "    batch_size: 8\n",
      "    max_seq_length: 256\n",
      "    doc_stride: 128\n",
      "    learning_rate: 0.0001\n",
      "    weight_decay: 0.01\n",
      "    adam_epsilon: 1e-08\n",
      "    adam_betas: (0.9, 0.999)\n",
      "    max_grad_norm: 1.0\n",
      "    num_iterations: 150\n",
      "    gradient_accumulation_steps: 8\n",
      "    eval_interval: 50\n",
      "    save_interval: 100\n",
      "    use_amp: True\n",
      "    empty_cache_interval: 25\n",
      "    num_workers: 0\n",
      "\n",
      "'timestamp':\n",
      "  Type: str\n",
      "  Value: 20250917_113554\n"
     ]
    }
   ],
   "source": [
    "# Detailed information about each key\n",
    "print(\"Detailed checkpoint contents:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for key in checkpoint.keys():\n",
    "    value = checkpoint[key]\n",
    "    print(f\"\\n'{key}':\")\n",
    "    \n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  Type: dict with {len(value)} items\")\n",
    "        if key == 'model_state_dict' or key == 'model':\n",
    "            # Show sample of model weights\n",
    "            sample_keys = list(value.keys())[:5]\n",
    "            print(f\"  Sample keys: {sample_keys}\")\n",
    "        elif len(value) < 200:  # For small dicts, show all keys\n",
    "            for k, v in value.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    print(f\"    {k}: tensor shape {v.shape}\")\n",
    "                else:\n",
    "                    print(f\"    {k}: {v}\")\n",
    "    \n",
    "    elif isinstance(value, list):\n",
    "        print(f\"  Type: list with {len(value)} items\")\n",
    "        print(f\"  Contents: {value}\")\n",
    "    \n",
    "    elif isinstance(value, torch.Tensor):\n",
    "        print(f\"  Type: tensor\")\n",
    "        print(f\"  Shape: {value.shape}\")\n",
    "        print(f\"  Dtype: {value.dtype}\")\n",
    "    \n",
    "    elif isinstance(value, (int, float, str)):\n",
    "        print(f\"  Type: {type(value).__name__}\")\n",
    "        print(f\"  Value: {value}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"  Type: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Configuration from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training stats/config loaded from JSON:\n",
      "==================================================\n",
      "\n",
      "Model Configuration:\n",
      "  quantization_bits: 8\n",
      "  n_layer: 6\n",
      "  n_embd: 768\n",
      "  n_head: 12\n",
      "\n",
      "Training Parameters:\n",
      "  train_split: train[:5000]\n",
      "  val_split: validation[:1000]\n",
      "  batch_size: 8\n",
      "  max_seq_length: 256\n",
      "  doc_stride: 128\n",
      "  learning_rate: 0.0001\n",
      "  weight_decay: 0.01\n",
      "  adam_epsilon: 1e-08\n",
      "  adam_betas: [0.9, 0.999]\n",
      "  max_grad_norm: 1.0\n",
      "  num_iterations: 150\n",
      "  gradient_accumulation_steps: 8\n",
      "  eval_interval: 50\n",
      "  save_interval: 100\n",
      "  use_amp: True\n",
      "  num_workers: 0\n"
     ]
    }
   ],
   "source": [
    "# Load the training stats JSON which contains configuration\n",
    "config_path = 'qat_training_stats_20250917_113554.json'\n",
    "\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        training_stats = json.load(f)\n",
    "    \n",
    "    print(\"Training stats/config loaded from JSON:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Show config if it exists\n",
    "    if 'model_config' in training_stats:\n",
    "        config = training_stats['model_config']\n",
    "        print(\"\\nModel Configuration:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Show training parameters\n",
    "    if 'training_config' in training_stats:\n",
    "        print(\"\\nTraining Parameters:\")\n",
    "        for key, value in training_stats['training_config'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"Config file not found at: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config\n",
    "from shared.models import SwitchableQATGPT2\n",
    "\n",
    "# Create GPT2Config from the JSON configuration\n",
    "if 'config' in training_stats:\n",
    "    config_dict = training_stats['config']\n",
    "    \n",
    "    # Create base GPT2 config\n",
    "    gpt2_config = GPT2Config(\n",
    "        vocab_size=config_dict.get('vocab_size', 50257),\n",
    "        n_positions=config_dict.get('n_positions', 1024),\n",
    "        n_embd=config_dict.get('n_embd', 768),\n",
    "        n_layer=config_dict.get('n_layer', 12),\n",
    "        n_head=config_dict.get('n_head', 12),\n",
    "    )\n",
    "    \n",
    "    # Add custom QAT attributes\n",
    "    for key, value in config_dict.items():\n",
    "        setattr(gpt2_config, key, value)\n",
    "    \n",
    "    print(\"GPT2Config created with attributes:\")\n",
    "    important_attrs = ['n_layer', 'n_embd', 'n_head', 'vocab_size', \n",
    "                      'lora_rank', 'lora_alpha', 'lora_dropout']\n",
    "    for attr in important_attrs:\n",
    "        if hasattr(gpt2_config, attr):\n",
    "            print(f\"  {attr}: {getattr(gpt2_config, attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "bit_widths = checkpoint.get('bit_widths', [4, 8, 16])\n",
    "print(f\"\\nCreating model with bit widths: {bit_widths}\")\n",
    "\n",
    "model = SwitchableQATGPT2(gpt2_config, bit_widths=bit_widths, initialize_weights=False)\n",
    "model = model.to(device)\n",
    "print(\"✓ Model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "elif 'model' in checkpoint:\n",
    "    state_dict = checkpoint['model']\n",
    "else:\n",
    "    # The checkpoint might be the state dict itself\n",
    "    state_dict = checkpoint\n",
    "\n",
    "# Load with strict=False to handle any mismatched keys\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "print(\"\\nModel weights loaded!\")\n",
    "if missing_keys:\n",
    "    print(f\"\\n⚠ Missing keys: {len(missing_keys)}\")\n",
    "    print(\"Sample missing keys:\", missing_keys[:5])\n",
    "if unexpected_keys:\n",
    "    print(f\"\\n⚠ Unexpected keys: {len(unexpected_keys)}\")\n",
    "    print(\"Sample unexpected keys:\", unexpected_keys[:5])\n",
    "\n",
    "if not missing_keys and not unexpected_keys:\n",
    "    print(\"✓ All weights loaded perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Test with a simple input\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "test_text = \"The quick brown fox\"\n",
    "inputs = tokenizer(test_text, return_tensors='pt').to(device)\n",
    "\n",
    "print(f\"Test input: '{test_text}'\")\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    # Test different bit widths if model supports switching\n",
    "    for bit_width in bit_widths:\n",
    "        if hasattr(model, 'set_precision'):\n",
    "            model.set_precision(bit_width)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[0]\n",
    "        \n",
    "        print(f\"\\nBit width {bit_width}:\")\n",
    "        print(f\"  Output shape: {logits.shape}\")\n",
    "        print(f\"  Output range: [{logits.min().item():.3f}, {logits.max().item():.3f}]\")\n",
    "        \n",
    "        # Get predicted next token\n",
    "        next_token_id = logits[0, -1].argmax().item()\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "        print(f\"  Predicted next token: '{next_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Alternative: Load Checkpoint for diagnose_model_issues.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows how to modify the checkpoint to work with diagnose_model_issues.py\n",
    "# The script expects 'config' key in the checkpoint\n",
    "\n",
    "# Create a new checkpoint with the expected structure\n",
    "if 'config' not in checkpoint and 'config' in training_stats:\n",
    "    print(\"Creating modified checkpoint with 'config' key...\")\n",
    "    \n",
    "    modified_checkpoint = checkpoint.copy()\n",
    "    modified_checkpoint['config'] = training_stats['config']\n",
    "    \n",
    "    # Save the modified checkpoint\n",
    "    modified_path = checkpoint_path.replace('.pth', '_with_config.pth')\n",
    "    torch.save(modified_checkpoint, modified_path)\n",
    "    \n",
    "    print(f\"✓ Saved modified checkpoint to: {modified_path}\")\n",
    "    print(\"\\nYou can now use this file with diagnose_model_issues.py:\")\n",
    "    print(f\"python test/diagnose_model_issues.py --model_path {modified_path}\")\n",
    "else:\n",
    "    print(\"Checkpoint already has 'config' key or config not found in JSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL)",
   "language": "python",
   "name": "wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
