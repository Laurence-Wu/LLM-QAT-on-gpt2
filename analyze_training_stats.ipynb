{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT Training Statistics Analysis\n",
    "\n",
    "This notebook analyzes the training statistics from the Quantization-Aware Training (QAT) of GPT-2.\n",
    "\n",
    "## Contents\n",
    "1. Load and inspect training data\n",
    "2. Training loss analysis\n",
    "3. Validation loss analysis\n",
    "4. Memory usage patterns\n",
    "5. Learning rate schedule\n",
    "6. Training efficiency metrics\n",
    "7. Convergence analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå File not found: qat_training_stats_20250915_012506.json.json\n",
      "Please ensure the training has been completed and stats file exists.\n"
     ]
    }
   ],
   "source": [
    "# Load training statistics\n",
    "def load_training_stats(filepath='qat_training_stats_20250915_012506.json'):\n",
    "    # Try to find the most recent file if default doesn't exist\n",
    "    import os\n",
    "    import glob\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Successfully loaded training stats from {filepath}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "        print(\"Please ensure the training has been completed and stats file exists.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ùå Invalid JSON in {filepath}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "stats = load_training_stats()\n",
    "\n",
    "if stats:\n",
    "    print(f\"\\nüìä Training Statistics Overview:\")\n",
    "    # Check for new format keys\n",
    "    if 'iteration_losses' in stats:\n",
    "        print(f\"  - Total iterations: {len(stats['iteration_losses'])}\")\n",
    "    elif 'iterations' in stats:\n",
    "        print(f\"  - Total iterations: {len(stats['iterations'])}\")\n",
    "    print(f\"  - Keys available: {list(stats.keys())}\")\n",
    "\n",
    "    # Show best validation loss if available\n",
    "    if 'best_val_loss' in stats:\n",
    "        print(f\"  - Best validation loss: {stats['best_val_loss']:.4f}\")\n",
    "    if 'best_iteration' in stats:\n",
    "        print(f\"  - Best iteration: {stats['best_iteration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "if stats:\n",
    "    # Handle both old and new format\n",
    "    if 'iteration_losses' in stats:\n",
    "        # New format - get the base length from iteration_losses\n",
    "        base_len = len(stats['iteration_losses'])\n",
    "        \n",
    "        # Helper function to normalize array length\n",
    "        def normalize_array(arr, target_len, default_val=None):\n",
    "            if arr is None:\n",
    "                return [default_val] * target_len\n",
    "            arr_len = len(arr)\n",
    "            if arr_len == target_len:\n",
    "                return arr\n",
    "            elif arr_len < target_len:\n",
    "                # Pad with default value\n",
    "                return list(arr) + [default_val] * (target_len - arr_len)\n",
    "            else:\n",
    "                # Truncate to target length\n",
    "                return arr[:target_len]\n",
    "        \n",
    "        # Normalize all arrays to base_len\n",
    "        df = pd.DataFrame({\n",
    "            'iteration': list(range(base_len)),\n",
    "            'train_loss': stats['iteration_losses'],\n",
    "            'val_loss': normalize_array(stats.get('validation_losses'), base_len),\n",
    "            'bit_width': normalize_array(stats.get('bit_width_usage') or stats.get('bit_width_history'), base_len),\n",
    "            'learning_rate': normalize_array(stats.get('learning_rates'), base_len),\n",
    "            'memory_mb': normalize_array(stats.get('memory_usage'), base_len)\n",
    "        })\n",
    "    else:\n",
    "        # Old format\n",
    "        base_len = len(stats.get('train_losses', []))\n",
    "        \n",
    "        def normalize_array(arr, target_len, default_val=None):\n",
    "            if arr is None:\n",
    "                return [default_val] * target_len\n",
    "            arr_len = len(arr)\n",
    "            if arr_len == target_len:\n",
    "                return arr\n",
    "            elif arr_len < target_len:\n",
    "                return list(arr) + [default_val] * (target_len - arr_len)\n",
    "            else:\n",
    "                return arr[:target_len]\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'iteration': stats.get('iterations', list(range(base_len))),\n",
    "            'train_loss': stats.get('train_losses', []),\n",
    "            'val_loss': normalize_array(stats.get('val_losses'), base_len),\n",
    "            'bit_width': normalize_array(stats.get('bit_width_history'), base_len),\n",
    "            'learning_rate': normalize_array(stats.get('learning_rates'), base_len),\n",
    "            'memory_mb': normalize_array(stats.get('memory_usage'), base_len)\n",
    "        })\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\nüìà Dataset Summary:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nüîç First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nüîç Last 5 rows:\")\n",
    "    display(df.tail())\n",
    "    \n",
    "    # Show best iteration info if available\n",
    "    if 'best_val_loss' in stats and 'best_iteration' in stats:\n",
    "        print(f\"\\nüèÜ Best Performance:\")\n",
    "        print(f\"  - Best validation loss: {stats['best_val_loss']:.4f}\")\n",
    "        print(f\"  - Achieved at iteration: {stats['best_iteration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Raw training loss\n",
    "    axes[0].plot(df['iteration'], df['train_loss'], alpha=0.6, label='Raw Loss')\n",
    "    \n",
    "    # Add smoothed line if enough data points\n",
    "    if len(df) > 50:\n",
    "        window_size = min(51, len(df) // 10 * 2 + 1)  # Ensure odd window size\n",
    "        smoothed = savgol_filter(df['train_loss'].dropna(), window_size, 3)\n",
    "        axes[0].plot(df['iteration'][:len(smoothed)], smoothed, 'r-', linewidth=2, label='Smoothed')\n",
    "    \n",
    "    axes[0].set_xlabel('Iteration')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].set_title('Training Loss Over Time')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Loss distribution\n",
    "    axes[1].hist(df['train_loss'].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].axvline(df['train_loss'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"train_loss\"].mean():.4f}')\n",
    "    axes[1].axvline(df['train_loss'].median(), color='green', linestyle='--', label=f'Median: {df[\"train_loss\"].median():.4f}')\n",
    "    axes[1].set_xlabel('Loss Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Training Loss Distribution')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate improvement metrics\n",
    "    initial_loss = df['train_loss'].iloc[:10].mean()\n",
    "    final_loss = df['train_loss'].iloc[-10:].mean()\n",
    "    improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "    \n",
    "    print(f\"\\nüìä Training Loss Metrics:\")\n",
    "    print(f\"  - Initial loss (first 10 iter): {initial_loss:.4f}\")\n",
    "    print(f\"  - Final loss (last 10 iter): {final_loss:.4f}\")\n",
    "    print(f\"  - Improvement: {improvement:.2f}%\")\n",
    "    print(f\"  - Minimum loss: {df['train_loss'].min():.4f} at iteration {df.loc[df['train_loss'].idxmin(), 'iteration']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validation Loss Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No validation losses key found in the statistics.\n"
     ]
    }
   ],
   "source": [
    "if stats and ('validation_losses' in stats or 'val_losses' in stats):\n",
    "    # Filter out None values for validation loss\n",
    "    val_df = df[df['val_loss'].notna()].copy()\n",
    "    \n",
    "    if not val_df.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Training vs Validation Loss\n",
    "        axes[0].plot(df['iteration'], df['train_loss'], alpha=0.6, label='Training Loss')\n",
    "        axes[0].scatter(val_df['iteration'], val_df['val_loss'], color='red', s=30, label='Validation Loss', zorder=5)\n",
    "        axes[0].plot(val_df['iteration'], val_df['val_loss'], 'r--', alpha=0.5)\n",
    "        \n",
    "        # Mark best validation loss if available\n",
    "        if 'best_val_loss' in stats and 'best_iteration' in stats:\n",
    "            axes[0].scatter(stats['best_iteration'], stats['best_val_loss'], \n",
    "                          color='green', s=100, marker='*', label=f'Best Val Loss: {stats[\"best_val_loss\"]:.4f}', zorder=10)\n",
    "        \n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].set_title('Training vs Validation Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Overfitting Analysis (Gap between train and val)\n",
    "        # Interpolate training loss at validation points\n",
    "        train_at_val = np.interp(val_df['iteration'], df['iteration'], df['train_loss'])\n",
    "        gap = val_df['val_loss'].values - train_at_val\n",
    "        \n",
    "        axes[1].plot(val_df['iteration'], gap, 'o-', color='purple')\n",
    "        axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[1].fill_between(val_df['iteration'], 0, gap, alpha=0.3, color='purple')\n",
    "        axes[1].set_xlabel('Iteration')\n",
    "        axes[1].set_ylabel('Validation - Training Loss')\n",
    "        axes[1].set_title('Generalization Gap (Overfitting Indicator)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(f\"\\nüìä Validation Metrics:\")\n",
    "        if 'best_val_loss' in stats and 'best_iteration' in stats:\n",
    "            print(f\"  - Best validation loss: {stats['best_val_loss']:.4f} at iteration {stats['best_iteration']}\")\n",
    "        else:\n",
    "            print(f\"  - Best validation loss: {val_df['val_loss'].min():.4f} at iteration {val_df.loc[val_df['val_loss'].idxmin(), 'iteration']}\")\n",
    "        print(f\"  - Average generalization gap: {gap.mean():.4f}\")\n",
    "        print(f\"  - Final generalization gap: {gap[-1]:.4f}\")\n",
    "        \n",
    "        if gap[-1] > gap[0]:\n",
    "            print(f\"  ‚ö†Ô∏è Warning: Increasing generalization gap (potential overfitting)\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Good: Stable or decreasing generalization gap\")\n",
    "    else:\n",
    "        print(\"No validation data available in the statistics.\")\n",
    "else:\n",
    "    print(\"No validation losses key found in the statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats and 'memory_usage' in stats:\n",
    "    mem_df = df[df['memory_mb'].notna()].copy()\n",
    "    \n",
    "    if not mem_df.empty:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Plot 1: Memory usage over time\n",
    "        axes[0].plot(mem_df['iteration'], mem_df['memory_mb'], 'b-', alpha=0.7)\n",
    "        axes[0].fill_between(mem_df['iteration'], mem_df['memory_mb'].min(), mem_df['memory_mb'], alpha=0.3)\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Memory Usage (MB)')\n",
    "        axes[0].set_title('GPU Memory Usage Over Time')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add memory leak detection line\n",
    "        z = np.polyfit(mem_df['iteration'], mem_df['memory_mb'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        axes[0].plot(mem_df['iteration'], p(mem_df['iteration']), \"r--\", alpha=0.5, label=f'Trend: {z[0]:.2f} MB/iter')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot 2: Memory distribution\n",
    "        axes[1].hist(mem_df['memory_mb'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[1].axvline(mem_df['memory_mb'].mean(), color='red', linestyle='--', label=f'Mean: {mem_df[\"memory_mb\"].mean():.0f} MB')\n",
    "        axes[1].set_xlabel('Memory Usage (MB)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Memory Usage Distribution')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Plot 3: Memory vs Loss correlation\n",
    "        axes[2].scatter(mem_df['memory_mb'], mem_df['train_loss'], alpha=0.5)\n",
    "        axes[2].set_xlabel('Memory Usage (MB)')\n",
    "        axes[2].set_ylabel('Training Loss')\n",
    "        axes[2].set_title('Memory Usage vs Training Loss')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        corr = mem_df['memory_mb'].corr(mem_df['train_loss'])\n",
    "        axes[2].text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=axes[2].transAxes, \n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Memory statistics\n",
    "        print(f\"\\nüíæ Memory Usage Statistics:\")\n",
    "        print(f\"  - Average memory: {mem_df['memory_mb'].mean():.0f} MB\")\n",
    "        print(f\"  - Peak memory: {mem_df['memory_mb'].max():.0f} MB\")\n",
    "        print(f\"  - Memory variance: {mem_df['memory_mb'].std():.0f} MB\")\n",
    "        print(f\"  - Memory growth rate: {z[0]:.4f} MB/iteration\")\n",
    "        \n",
    "        if abs(z[0]) < 0.1:\n",
    "            print(f\"  ‚úÖ No significant memory leak detected\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è Potential memory leak: {z[0]:.4f} MB/iteration growth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning Rate Schedule Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats and 'learning_rates' in stats:\n",
    "    lr_df = df[df['learning_rate'].notna()].copy()\n",
    "    \n",
    "    if not lr_df.empty:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Learning rate schedule\n",
    "        axes[0].plot(lr_df['iteration'], lr_df['learning_rate'], 'g-', linewidth=2)\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Learning Rate')\n",
    "        axes[0].set_title('Learning Rate Schedule (Cosine Annealing)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_yscale('log')\n",
    "        \n",
    "        # Plot 2: Learning rate vs Loss\n",
    "        scatter = axes[1].scatter(lr_df['learning_rate'], lr_df['train_loss'], \n",
    "                                 c=lr_df['iteration'], cmap='viridis', alpha=0.6)\n",
    "        axes[1].set_xlabel('Learning Rate')\n",
    "        axes[1].set_ylabel('Training Loss')\n",
    "        axes[1].set_title('Learning Rate vs Training Loss')\n",
    "        axes[1].set_xscale('log')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "        cbar.set_label('Iteration')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüéØ Learning Rate Statistics:\")\n",
    "        print(f\"  - Initial LR: {lr_df['learning_rate'].iloc[0]:.6f}\")\n",
    "        print(f\"  - Final LR: {lr_df['learning_rate'].iloc[-1]:.6f}\")\n",
    "        print(f\"  - LR reduction: {(1 - lr_df['learning_rate'].iloc[-1]/lr_df['learning_rate'].iloc[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Efficiency Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats:\n",
    "    # Calculate various efficiency metrics\n",
    "    \n",
    "    # Loss reduction rate\n",
    "    window = min(50, len(df) // 10)\n",
    "    loss_changes = df['train_loss'].rolling(window=window).mean().diff()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss reduction rate\n",
    "    axes[0, 0].plot(df['iteration'][window:], loss_changes[window:], alpha=0.7)\n",
    "    axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Loss Change Rate')\n",
    "    axes[0, 0].set_title(f'Training Loss Change Rate (window={window})')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cumulative loss reduction\n",
    "    cumulative_improvement = (df['train_loss'].iloc[0] - df['train_loss']) / df['train_loss'].iloc[0] * 100\n",
    "    axes[0, 1].plot(df['iteration'], cumulative_improvement, 'b-', linewidth=2)\n",
    "    axes[0, 1].fill_between(df['iteration'], 0, cumulative_improvement, alpha=0.3)\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Improvement (%)')\n",
    "    axes[0, 1].set_title('Cumulative Loss Improvement')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Training stability (loss variance over windows)\n",
    "    loss_variance = df['train_loss'].rolling(window=window).std()\n",
    "    axes[1, 0].plot(df['iteration'][window:], loss_variance[window:], 'orange', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Iteration')\n",
    "    axes[1, 0].set_ylabel('Loss Std Dev')\n",
    "    axes[1, 0].set_title(f'Training Stability (Loss Variance, window={window})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Iterations per loss unit\n",
    "    loss_ranges = np.arange(df['train_loss'].min(), df['train_loss'].max(), 0.1)\n",
    "    iterations_needed = []\n",
    "    for i in range(len(loss_ranges)-1):\n",
    "        mask = (df['train_loss'] >= loss_ranges[i]) & (df['train_loss'] < loss_ranges[i+1])\n",
    "        iterations_needed.append(mask.sum())\n",
    "    \n",
    "    axes[1, 1].bar(loss_ranges[:-1], iterations_needed, width=0.09, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Loss Range')\n",
    "    axes[1, 1].set_ylabel('Number of Iterations')\n",
    "    axes[1, 1].set_title('Iterations Spent in Each Loss Range')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    early_improvement = cumulative_improvement.iloc[len(df)//4]\n",
    "    late_improvement = cumulative_improvement.iloc[-1] - cumulative_improvement.iloc[3*len(df)//4]\n",
    "    \n",
    "    print(f\"\\n‚ö° Training Efficiency Metrics:\")\n",
    "    print(f\"  - Early training improvement (first 25%): {early_improvement:.2f}%\")\n",
    "    print(f\"  - Late training improvement (last 25%): {late_improvement:.2f}%\")\n",
    "    print(f\"  - Average loss variance: {loss_variance.mean():.4f}\")\n",
    "    print(f\"  - Training stability score: {1/(1+loss_variance.mean()):.3f} (higher is better)\")\n",
    "    \n",
    "    if early_improvement > late_improvement * 2:\n",
    "        print(f\"  ‚úÖ Efficient early training - most learning happened early\")\n",
    "    else:\n",
    "        print(f\"  üìà Continued learning throughout training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats:\n",
    "    # Convergence analysis\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Loss convergence with exponential fit\n",
    "    from scipy.optimize import curve_fit\n",
    "    \n",
    "    def exp_decay(x, a, b, c):\n",
    "        return a * np.exp(-b * x) + c\n",
    "    \n",
    "    try:\n",
    "        # Fit exponential decay\n",
    "        x_data = df['iteration'].values\n",
    "        y_data = df['train_loss'].values\n",
    "        \n",
    "        # Initial guess\n",
    "        p0 = [y_data[0] - y_data[-1], 0.001, y_data[-1]]\n",
    "        popt, pcov = curve_fit(exp_decay, x_data, y_data, p0=p0, maxfev=5000)\n",
    "        \n",
    "        # Plot actual and fitted\n",
    "        axes[0].plot(x_data, y_data, 'b-', alpha=0.5, label='Actual Loss')\n",
    "        axes[0].plot(x_data, exp_decay(x_data, *popt), 'r-', linewidth=2, \n",
    "                    label=f'Fitted: {popt[0]:.3f}*exp(-{popt[1]:.4f}*x)+{popt[2]:.3f}')\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Training Loss')\n",
    "        axes[0].set_title('Loss Convergence with Exponential Fit')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Estimate convergence point (where decay is < 0.001 per iteration)\n",
    "        convergence_iter = -np.log(0.001 / popt[0]) / popt[1] if popt[1] > 0 else np.inf\n",
    "        if convergence_iter < len(df) * 2:\n",
    "            axes[0].axvline(x=convergence_iter, color='green', linestyle='--', \n",
    "                          label=f'Est. Convergence: {convergence_iter:.0f}')\n",
    "            axes[0].legend()\n",
    "            \n",
    "    except:\n",
    "        axes[0].plot(df['iteration'], df['train_loss'], 'b-', alpha=0.7)\n",
    "        axes[0].set_xlabel('Iteration')\n",
    "        axes[0].set_ylabel('Training Loss')\n",
    "        axes[0].set_title('Training Loss (Exponential fit failed)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Gradient magnitude estimate (loss changes)\n",
    "    grad_estimate = np.abs(df['train_loss'].diff())\n",
    "    window_size = min(51, len(df) // 10 * 2 + 1)\n",
    "    if len(grad_estimate) > window_size:\n",
    "        grad_smooth = savgol_filter(grad_estimate.dropna(), window_size, 3)\n",
    "        axes[1].plot(df['iteration'][1:len(grad_smooth)+1], grad_smooth, 'purple', linewidth=2)\n",
    "    axes[1].set_xlabel('Iteration')\n",
    "    axes[1].set_ylabel('|Loss Change|')\n",
    "    axes[1].set_title('Estimated Gradient Magnitude (Loss Change Rate)')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Convergence metrics\n",
    "    last_100_std = df['train_loss'].iloc[-100:].std() if len(df) > 100 else df['train_loss'].std()\n",
    "    last_100_mean = df['train_loss'].iloc[-100:].mean() if len(df) > 100 else df['train_loss'].mean()\n",
    "    \n",
    "    print(f\"\\nüéØ Convergence Analysis:\")\n",
    "    print(f\"  - Final loss stability (std of last 100): {last_100_std:.5f}\")\n",
    "    print(f\"  - Final loss mean (last 100): {last_100_mean:.4f}\")\n",
    "    \n",
    "    if 'popt' in locals():\n",
    "        print(f\"  - Exponential decay rate: {popt[1]:.5f}\")\n",
    "        print(f\"  - Asymptotic loss (predicted): {popt[2]:.4f}\")\n",
    "        if convergence_iter < len(df) * 2:\n",
    "            print(f\"  - Estimated convergence iteration: {convergence_iter:.0f}\")\n",
    "            if convergence_iter < len(df):\n",
    "                print(f\"  ‚úÖ Model has likely converged\")\n",
    "            else:\n",
    "                print(f\"  üìà Model approaching convergence\")\n",
    "    \n",
    "    if last_100_std < 0.01:\n",
    "        print(f\"  ‚úÖ Training has converged (low variance in final iterations)\")\n",
    "    else:\n",
    "        print(f\"  üìà Training still showing variation (may benefit from more iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stats:\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìä TRAINING SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Key metrics\n",
    "    print(\"\\nüîë Key Metrics:\")\n",
    "    print(f\"  - Total iterations: {len(df)}\")\n",
    "    print(f\"  - Final training loss: {df['train_loss'].iloc[-1]:.4f}\")\n",
    "    print(f\"  - Best training loss: {df['train_loss'].min():.4f}\")\n",
    "    print(f\"  - Total improvement: {(1 - df['train_loss'].iloc[-1]/df['train_loss'].iloc[0])*100:.2f}%\")\n",
    "    \n",
    "    # Check for best_val_loss and best_iteration in stats directly\n",
    "    if 'best_val_loss' in stats and 'best_iteration' in stats:\n",
    "        print(f\"  - Best validation loss: {stats['best_val_loss']:.4f}\")\n",
    "        print(f\"  - Best iteration: {stats['best_iteration']}\")\n",
    "    elif ('validation_losses' in stats or 'val_losses' in stats) and any(v is not None for v in (stats.get('validation_losses', []) or stats.get('val_losses', []))):\n",
    "        val_df = df[df['val_loss'].notna()]\n",
    "        if not val_df.empty:\n",
    "            print(f\"  - Best validation loss: {val_df['val_loss'].min():.4f}\")\n",
    "            print(f\"  - Final validation loss: {val_df['val_loss'].iloc[-1]:.4f}\")\n",
    "    \n",
    "    # Bit-width information if available\n",
    "    if 'bit_width_usage' in stats or 'bit_width' in df.columns:\n",
    "        bit_data = stats.get('bit_width_usage', df['bit_width'].dropna().tolist() if 'bit_width' in df.columns else [])\n",
    "        if bit_data:\n",
    "            print(f\"  - Bit-width range: {min(bit_data)}-{max(bit_data)} bits\")\n",
    "            print(f\"  - Average bit-width: {np.mean(bit_data):.2f} bits\")\n",
    "    \n",
    "    # Training quality assessment\n",
    "    print(\"\\n‚úÖ Training Quality Assessment:\")\n",
    "    \n",
    "    quality_score = 0\n",
    "    max_score = 0\n",
    "    \n",
    "    # Check 1: Loss improvement\n",
    "    improvement = (1 - df['train_loss'].iloc[-1]/df['train_loss'].iloc[0])\n",
    "    max_score += 1\n",
    "    if improvement > 0.3:\n",
    "        print(f\"  ‚úì Good loss improvement ({improvement*100:.1f}%)\")\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó Limited loss improvement ({improvement*100:.1f}%)\")\n",
    "    \n",
    "    # Check 2: Training stability\n",
    "    final_std = df['train_loss'].iloc[-100:].std() if len(df) > 100 else df['train_loss'].std()\n",
    "    max_score += 1\n",
    "    if final_std < 0.05:\n",
    "        print(f\"  ‚úì Stable training (std={final_std:.4f})\")\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó Unstable training (std={final_std:.4f})\")\n",
    "    \n",
    "    # Check 3: No memory leaks\n",
    "    if 'memory_usage' in stats:\n",
    "        mem_df = df[df['memory_mb'].notna()]\n",
    "        if not mem_df.empty:\n",
    "            z = np.polyfit(mem_df['iteration'], mem_df['memory_mb'], 1)\n",
    "            max_score += 1\n",
    "            if abs(z[0]) < 0.5:\n",
    "                print(f\"  ‚úì No memory leaks detected\")\n",
    "                quality_score += 1\n",
    "            else:\n",
    "                print(f\"  ‚úó Potential memory leak ({z[0]:.2f} MB/iter)\")\n",
    "    \n",
    "    # Check 4: Convergence\n",
    "    max_score += 1\n",
    "    if final_std < 0.01:\n",
    "        print(f\"  ‚úì Training converged\")\n",
    "        quality_score += 1\n",
    "    else:\n",
    "        print(f\"  ‚úó Training not fully converged\")\n",
    "    \n",
    "    # Check 5: Validation performance (if best_val_loss available)\n",
    "    if 'best_val_loss' in stats:\n",
    "        max_score += 1\n",
    "        # Check if validation loss is reasonable compared to training loss\n",
    "        best_train = df['train_loss'].min()\n",
    "        gap_ratio = (stats['best_val_loss'] - best_train) / best_train if best_train > 0 else float('inf')\n",
    "        if gap_ratio < 0.2:  # Less than 20% gap\n",
    "            print(f\"  ‚úì Good generalization (gap ratio: {gap_ratio:.2%})\")\n",
    "            quality_score += 1\n",
    "        else:\n",
    "            print(f\"  ‚úó Large generalization gap (gap ratio: {gap_ratio:.2%})\")\n",
    "    \n",
    "    # Overall score\n",
    "    print(f\"\\nüèÜ Overall Training Quality Score: {quality_score}/{max_score} ({quality_score/max_score*100:.0f}%)\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° Recommendations:\")\n",
    "    \n",
    "    if final_std > 0.01:\n",
    "        print(\"  ‚Ä¢ Consider training for more iterations to achieve full convergence\")\n",
    "    \n",
    "    if 'validation_losses' in stats or 'val_losses' in stats:\n",
    "        val_df = df[df['val_loss'].notna()]\n",
    "        if not val_df.empty:\n",
    "            train_at_val = np.interp(val_df['iteration'], df['iteration'], df['train_loss'])\n",
    "            final_gap = val_df['val_loss'].iloc[-1] - train_at_val[-1]\n",
    "            if final_gap > 0.5:\n",
    "                print(\"  ‚Ä¢ High generalization gap - consider regularization or more data\")\n",
    "    \n",
    "    if improvement < 0.2:\n",
    "        print(\"  ‚Ä¢ Limited improvement - consider adjusting learning rate or model architecture\")\n",
    "    \n",
    "    if 'memory_usage' in stats and 'z' in locals() and abs(z[0]) > 0.5:\n",
    "        print(\"  ‚Ä¢ Memory leak detected - review memory management in training loop\")\n",
    "    \n",
    "    if 'best_iteration' in stats and stats['best_iteration'] < len(df) * 0.7:\n",
    "        print(f\"  ‚Ä¢ Early stopping might be beneficial - best performance at iteration {stats['best_iteration']} ({stats['best_iteration']/len(df)*100:.1f}% through training)\")\n",
    "    \n",
    "    if quality_score == max_score:\n",
    "        print(\"  ‚Ä¢ Excellent training run! Model is ready for deployment.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. CPT (Cyclic Precision Training) Analysis\n",
    "\n",
    "This section analyzes statistics from Cyclic Precision Training if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found CPT stats file: cpt_training_stats_20250915_004044.json\n",
      "‚ùå Error loading CPT stats: name 'json' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Load CPT training statistics\n",
    "def load_cpt_stats(filepath=None):\n",
    "    \"\"\"Load CPT training statistics from JSON file.\"\"\"\n",
    "    if filepath is None:\n",
    "        # Try to find the most recent CPT stats file\n",
    "        import glob\n",
    "        import os\n",
    "        cpt_files = glob.glob('cpt_training_stats_*.json')\n",
    "        if not cpt_files:\n",
    "            cpt_files = glob.glob('part2_cyclic_precision/cpt_training_stats_*.json')\n",
    "        \n",
    "        if cpt_files:\n",
    "            # Get the most recent file\n",
    "            filepath = max(cpt_files, key=os.path.getctime)\n",
    "            print(f\"Found CPT stats file: {filepath}\")\n",
    "        else:\n",
    "            print(\"No CPT training statistics file found\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Successfully loaded CPT stats from {filepath}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CPT stats: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try to load CPT stats\n",
    "cpt_stats = load_cpt_stats()\n",
    "\n",
    "if cpt_stats:\n",
    "    print(f\"\\nüìä CPT Training Statistics Overview:\")\n",
    "    print(f\"  - Total iterations: {len(cpt_stats.get('iteration_losses', []))}\")\n",
    "    print(f\"  - Keys available: {list(cpt_stats.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze CPT bit-width cycling patterns\n",
    "if cpt_stats:\n",
    "    # Check for bit_width_usage (new format) or bit_width_history (old format)\n",
    "    bit_history = cpt_stats.get('bit_width_usage', cpt_stats.get('bit_width_history', []))\n",
    "    \n",
    "    if bit_history:\n",
    "        iterations = list(range(len(bit_history)))\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Bit-width over time\n",
    "        axes[0, 0].plot(iterations, bit_history, 'g-', linewidth=2, alpha=0.7)\n",
    "        axes[0, 0].set_xlabel('Iteration')\n",
    "        axes[0, 0].set_ylabel('Bit Width')\n",
    "        axes[0, 0].set_title('Bit-Width Cycling Pattern')\n",
    "        axes[0, 0].set_ylim([0, max(bit_history) + 1])\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_yticks(sorted(set(bit_history)))\n",
    "        \n",
    "        # Plot 2: Loss vs Bit-width\n",
    "        losses = cpt_stats.get('iteration_losses', cpt_stats.get('train_losses', []))[:len(bit_history)]\n",
    "        \n",
    "        if losses:\n",
    "            # Create scatter plot colored by iteration\n",
    "            scatter = axes[0, 1].scatter(bit_history, losses, c=iterations, \n",
    "                                        cmap='viridis', alpha=0.6, s=20)\n",
    "            axes[0, 1].set_xlabel('Bit Width')\n",
    "            axes[0, 1].set_ylabel('Training Loss')\n",
    "            axes[0, 1].set_title('Loss vs Bit-Width')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            plt.colorbar(scatter, ax=axes[0, 1], label='Iteration')\n",
    "            \n",
    "            # Calculate average loss per bit-width\n",
    "            bit_values = sorted(set(bit_history))\n",
    "            avg_losses = []\n",
    "            for bit in bit_values:\n",
    "                bit_losses = [losses[i] for i, b in enumerate(bit_history) if b == bit]\n",
    "                avg_losses.append(np.mean(bit_losses))\n",
    "            \n",
    "            axes[1, 0].bar(bit_values, avg_losses, alpha=0.7, edgecolor='black')\n",
    "            axes[1, 0].set_xlabel('Bit Width')\n",
    "            axes[1, 0].set_ylabel('Average Loss')\n",
    "            axes[1, 0].set_title('Average Loss per Bit-Width')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 4: Bit-width distribution\n",
    "            axes[1, 1].hist(bit_history, bins=len(set(bit_history)), \n",
    "                           alpha=0.7, edgecolor='black', color='orange')\n",
    "            axes[1, 1].set_xlabel('Bit Width')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].set_title('Bit-Width Distribution')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"\\nüîÑ CPT Bit-Width Statistics:\")\n",
    "        print(f\"  - Unique bit widths used: {sorted(set(bit_history))}\")\n",
    "        print(f\"  - Most common bit width: {max(set(bit_history), key=bit_history.count)}\")\n",
    "        print(f\"  - Average bit width: {np.mean(bit_history):.2f}\")\n",
    "        \n",
    "        if losses:\n",
    "            print(f\"\\nüìä Loss per Bit-Width:\")\n",
    "            for bit, avg_loss in zip(bit_values, avg_losses):\n",
    "                print(f\"  - {bit}-bit: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Show best performance if available\n",
    "        if 'best_val_loss' in cpt_stats and 'best_iteration' in cpt_stats:\n",
    "            print(f\"\\nüèÜ Best CPT Performance:\")\n",
    "            print(f\"  - Best validation loss: {cpt_stats['best_val_loss']:.4f}\")\n",
    "            print(f\"  - Achieved at iteration: {cpt_stats['best_iteration']}\")\n",
    "            if cpt_stats['best_iteration'] < len(bit_history):\n",
    "                print(f\"  - Bit-width at best iteration: {bit_history[cpt_stats['best_iteration']]}\")\n",
    "elif stats and 'bit_width' in df.columns:\n",
    "    # If bit_width data is in the main stats\n",
    "    bit_df = df[df['bit_width'].notna()].copy()\n",
    "    if not bit_df.empty:\n",
    "        print(\"\\nüîÑ Bit-Width Usage Analysis:\")\n",
    "        print(f\"  - Unique bit widths: {sorted(bit_df['bit_width'].unique())}\")\n",
    "        print(f\"  - Average bit width: {bit_df['bit_width'].mean():.2f}\")\n",
    "        \n",
    "        # Plot bit width over time if available\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        ax.plot(bit_df['iteration'], bit_df['bit_width'], 'g-', linewidth=2, alpha=0.7)\n",
    "        ax.set_xlabel('Iteration')\n",
    "        ax.set_ylabel('Bit Width')\n",
    "        ax.set_title('Bit-Width Usage Over Training')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL)",
   "language": "python",
   "name": "wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
