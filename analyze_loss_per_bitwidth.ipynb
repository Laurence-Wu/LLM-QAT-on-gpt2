{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Per Bit-Width Analysis for QAT Training\n",
    "\n",
    "This notebook analyzes the training loss patterns for different bit-widths during Quantization-Aware Training (QAT) and Cyclic Precision Training (CPT).\n",
    "\n",
    "## Contents\n",
    "1. Load and inspect training data\n",
    "2. Loss per bit-width analysis\n",
    "3. Bit-width switching patterns\n",
    "4. Performance comparison across bit-widths\n",
    "5. Optimal bit-width determination\n",
    "6. Convergence analysis per bit-width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_training_stats():\n",
    "    \"\"\"Load all available training statistics files.\"\"\"\n",
    "    stats_files = {}\n",
    "    \n",
    "    # Find QAT training stats\n",
    "    qat_files = glob.glob('qat_training_stats_*.json')\n",
    "    if qat_files:\n",
    "        latest_qat = max(qat_files, key=os.path.getctime)\n",
    "        try:\n",
    "            with open(latest_qat, 'r') as f:\n",
    "                stats_files['QAT'] = json.load(f)\n",
    "                print(f\"✅ Loaded QAT stats from: {latest_qat}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading QAT stats: {e}\")\n",
    "    \n",
    "    # Find CPT training stats\n",
    "    cpt_files = glob.glob('cpt_training_stats_*.json')\n",
    "    if not cpt_files:\n",
    "        cpt_files = glob.glob('part2_cyclic_precision/cpt_training_stats_*.json')\n",
    "    \n",
    "    if cpt_files:\n",
    "        latest_cpt = max(cpt_files, key=os.path.getctime)\n",
    "        try:\n",
    "            with open(latest_cpt, 'r') as f:\n",
    "                stats_files['CPT'] = json.load(f)\n",
    "                print(f\"✅ Loaded CPT stats from: {latest_cpt}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading CPT stats: {e}\")\n",
    "    \n",
    "    return stats_files\n",
    "\n",
    "# Load all available statistics\n",
    "all_stats = load_all_training_stats()\n",
    "\n",
    "if all_stats:\n",
    "    print(f\"\\n📊 Loaded {len(all_stats)} training statistics files\")\n",
    "    for name, stats in all_stats.items():\n",
    "        print(f\"\\n{name} Statistics:\")\n",
    "        print(f\"  - Keys: {list(stats.keys())[:10]}...\" if len(stats.keys()) > 10 else f\"  - Keys: {list(stats.keys())}\")\n",
    "        if 'iteration_losses' in stats:\n",
    "            print(f\"  - Iterations: {len(stats['iteration_losses'])}\")\n",
    "        if 'losses_per_bit' in stats:\n",
    "            print(f\"  - Bit-widths tracked: {list(stats['losses_per_bit'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detailed Loss Per Bit-Width Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_losses_per_bit(stats, name=\"Training\"):\n",
    "    \"\"\"Analyze losses for each bit-width.\"\"\"\n",
    "    \n",
    "    # Check if we have losses_per_bit data\n",
    "    if 'losses_per_bit' in stats and stats['losses_per_bit']:\n",
    "        losses_per_bit = stats['losses_per_bit']\n",
    "        \n",
    "        # Convert to proper format (handle both string and int keys)\n",
    "        losses_dict = {}\n",
    "        for key, value in losses_per_bit.items():\n",
    "            bit_width = int(key) if isinstance(key, str) else key\n",
    "            if value:  # Only include non-empty lists\n",
    "                losses_dict[bit_width] = value\n",
    "        \n",
    "        if losses_dict:\n",
    "            print(f\"\\n📊 {name} - Loss Statistics per Bit-Width:\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            bit_stats = []\n",
    "            for bit_width in sorted(losses_dict.keys()):\n",
    "                losses = losses_dict[bit_width]\n",
    "                if losses:\n",
    "                    stats_row = {\n",
    "                        'Bit Width': bit_width,\n",
    "                        'Samples': len(losses),\n",
    "                        'Mean Loss': np.mean(losses),\n",
    "                        'Std Dev': np.std(losses),\n",
    "                        'Min Loss': np.min(losses),\n",
    "                        'Max Loss': np.max(losses),\n",
    "                        'Final Loss': losses[-1] if losses else np.nan\n",
    "                    }\n",
    "                    bit_stats.append(stats_row)\n",
    "            \n",
    "            df_stats = pd.DataFrame(bit_stats)\n",
    "            display(df_stats.round(4))\n",
    "            \n",
    "            return losses_dict, df_stats\n",
    "    \n",
    "    # Alternative: Calculate from bit_width_usage and iteration_losses\n",
    "    elif 'bit_width_usage' in stats and 'iteration_losses' in stats:\n",
    "        bit_widths = stats['bit_width_usage']\n",
    "        losses = stats['iteration_losses']\n",
    "        \n",
    "        # Group losses by bit-width\n",
    "        losses_dict = {}\n",
    "        for i, (bit, loss) in enumerate(zip(bit_widths, losses)):\n",
    "            if bit is not None:\n",
    "                bit = int(bit)\n",
    "                if bit not in losses_dict:\n",
    "                    losses_dict[bit] = []\n",
    "                losses_dict[bit].append(loss)\n",
    "        \n",
    "        if losses_dict:\n",
    "            print(f\"\\n📊 {name} - Loss Statistics per Bit-Width (Calculated):\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            bit_stats = []\n",
    "            for bit_width in sorted(losses_dict.keys()):\n",
    "                losses = losses_dict[bit_width]\n",
    "                stats_row = {\n",
    "                    'Bit Width': bit_width,\n",
    "                    'Samples': len(losses),\n",
    "                    'Mean Loss': np.mean(losses),\n",
    "                    'Std Dev': np.std(losses),\n",
    "                    'Min Loss': np.min(losses),\n",
    "                    'Max Loss': np.max(losses),\n",
    "                    'Final Loss': losses[-1]\n",
    "                }\n",
    "                bit_stats.append(stats_row)\n",
    "            \n",
    "            df_stats = pd.DataFrame(bit_stats)\n",
    "            display(df_stats.round(4))\n",
    "            \n",
    "            return losses_dict, df_stats\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# Analyze losses for each training type\n",
    "results = {}\n",
    "for name, stats in all_stats.items():\n",
    "    losses_dict, df_stats = analyze_losses_per_bit(stats, name)\n",
    "    if losses_dict:\n",
    "        results[name] = {'losses': losses_dict, 'stats': df_stats}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Loss Distributions per Bit-Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_distributions(results):\n",
    "    \"\"\"Create comprehensive visualizations for loss per bit-width.\"\"\"\n",
    "    \n",
    "    for training_name, data in results.items():\n",
    "        losses_dict = data['losses']\n",
    "        \n",
    "        # Determine subplot layout\n",
    "        n_bits = len(losses_dict)\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        fig.suptitle(f'{training_name} - Loss Analysis per Bit-Width', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Create 4 subplots\n",
    "        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # 1. Box plot comparison\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        box_data = [losses_dict[bit] for bit in sorted(losses_dict.keys())]\n",
    "        box_labels = [f\"{bit}-bit\" for bit in sorted(losses_dict.keys())]\n",
    "        bp = ax1.boxplot(box_data, labels=box_labels, patch_artist=True)\n",
    "        \n",
    "        # Color boxes by bit-width\n",
    "        colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(box_data)))\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        ax1.set_xlabel('Bit Width')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Loss Distribution Comparison')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Loss trajectory per bit-width\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        for bit in sorted(losses_dict.keys()):\n",
    "            losses = losses_dict[bit]\n",
    "            iterations = range(len(losses))\n",
    "            ax2.plot(iterations, losses, label=f'{bit}-bit', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax2.set_xlabel('Sample Index')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Loss Trajectory per Bit-Width')\n",
    "        ax2.legend(loc='best')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Mean loss comparison with error bars\n",
    "        ax3 = fig.add_subplot(gs[1, 0])\n",
    "        bit_widths = sorted(losses_dict.keys())\n",
    "        means = [np.mean(losses_dict[bit]) for bit in bit_widths]\n",
    "        stds = [np.std(losses_dict[bit]) for bit in bit_widths]\n",
    "        \n",
    "        bars = ax3.bar(bit_widths, means, yerr=stds, capsize=5, \n",
    "                       color=colors, edgecolor='black', linewidth=1.5)\n",
    "        ax3.set_xlabel('Bit Width')\n",
    "        ax3.set_ylabel('Mean Loss')\n",
    "        ax3.set_title('Average Loss per Bit-Width (with std dev)')\n",
    "        ax3.set_xticks(bit_widths)\n",
    "        ax3.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, mean, std in zip(bars, means, stds):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + std,\n",
    "                    f'{mean:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 4. Convergence rate per bit-width\n",
    "        ax4 = fig.add_subplot(gs[1, 1])\n",
    "        for bit in sorted(losses_dict.keys()):\n",
    "            losses = losses_dict[bit]\n",
    "            if len(losses) > 10:\n",
    "                # Calculate rolling mean for smoother convergence curve\n",
    "                window = min(10, len(losses) // 5)\n",
    "                rolling_mean = pd.Series(losses).rolling(window=window, center=True).mean()\n",
    "                \n",
    "                # Calculate convergence rate (negative of loss derivative)\n",
    "                convergence_rate = -np.gradient(rolling_mean.dropna())\n",
    "                x_axis = range(len(convergence_rate))\n",
    "                \n",
    "                ax4.plot(x_axis, convergence_rate, label=f'{bit}-bit', alpha=0.7, linewidth=2)\n",
    "        \n",
    "        ax4.set_xlabel('Sample Index')\n",
    "        ax4.set_ylabel('Convergence Rate (Loss Reduction)')\n",
    "        ax4.set_title('Convergence Rate per Bit-Width')\n",
    "        ax4.legend(loc='best')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "if results:\n",
    "    plot_loss_distributions(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bit-Width Switching Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bit_switching_patterns(stats, name=\"Training\"):\n",
    "    \"\"\"Analyze bit-width switching patterns and their impact on loss.\"\"\"\n",
    "    \n",
    "    if 'bit_width_usage' not in stats or 'iteration_losses' not in stats:\n",
    "        print(f\"No bit-width usage data available for {name}\")\n",
    "        return\n",
    "    \n",
    "    bit_widths = stats['bit_width_usage']\n",
    "    losses = stats['iteration_losses']\n",
    "    \n",
    "    # Ensure equal length\n",
    "    min_len = min(len(bit_widths), len(losses))\n",
    "    bit_widths = bit_widths[:min_len]\n",
    "    losses = losses[:min_len]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    fig.suptitle(f'{name} - Bit-Width Switching Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Bit-width usage over time\n",
    "    axes[0, 0].plot(range(len(bit_widths)), bit_widths, 'g-', linewidth=1.5, alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Bit Width')\n",
    "    axes[0, 0].set_title('Bit-Width Schedule Over Training')\n",
    "    axes[0, 0].set_yticks(sorted(set([b for b in bit_widths if b is not None])))\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss colored by bit-width\n",
    "    scatter = axes[0, 1].scatter(range(len(losses)), losses, \n",
    "                                 c=bit_widths, cmap='viridis', \n",
    "                                 s=10, alpha=0.6)\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Training Loss (colored by bit-width)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0, 1], label='Bit Width')\n",
    "    \n",
    "    # 3. Transition impact analysis\n",
    "    transitions = []\n",
    "    transition_losses = []\n",
    "    for i in range(1, len(bit_widths)):\n",
    "        if bit_widths[i] != bit_widths[i-1] and bit_widths[i] is not None and bit_widths[i-1] is not None:\n",
    "            transitions.append(i)\n",
    "            if i < len(losses):\n",
    "                # Calculate loss change during transition\n",
    "                loss_change = losses[i] - losses[i-1] if i > 0 else 0\n",
    "                transition_losses.append(loss_change)\n",
    "    \n",
    "    if transitions:\n",
    "        axes[1, 0].scatter(transitions, transition_losses, alpha=0.6, s=30)\n",
    "        axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[1, 0].set_xlabel('Iteration')\n",
    "        axes[1, 0].set_ylabel('Loss Change at Transition')\n",
    "        axes[1, 0].set_title(f'Loss Impact of Bit-Width Transitions (n={len(transitions)})')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add average line\n",
    "        avg_impact = np.mean(transition_losses)\n",
    "        axes[1, 0].axhline(y=avg_impact, color='blue', linestyle=':', \n",
    "                          label=f'Avg: {avg_impact:.4f}')\n",
    "        axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Bit-width frequency histogram\n",
    "    unique_bits = [b for b in bit_widths if b is not None]\n",
    "    axes[1, 1].hist(unique_bits, bins=len(set(unique_bits)), \n",
    "                   edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_xlabel('Bit Width')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Bit-Width Usage Distribution')\n",
    "    axes[1, 1].set_xticks(sorted(set(unique_bits)))\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 5. Loss improvement per bit-width\n",
    "    bit_improvements = {}\n",
    "    for bit in set(unique_bits):\n",
    "        bit_indices = [i for i, b in enumerate(bit_widths) if b == bit]\n",
    "        if len(bit_indices) > 1:\n",
    "            bit_losses = [losses[i] for i in bit_indices if i < len(losses)]\n",
    "            if len(bit_losses) > 1:\n",
    "                improvement = (bit_losses[0] - bit_losses[-1]) / bit_losses[0] * 100\n",
    "                bit_improvements[bit] = improvement\n",
    "    \n",
    "    if bit_improvements:\n",
    "        bits = sorted(bit_improvements.keys())\n",
    "        improvements = [bit_improvements[b] for b in bits]\n",
    "        \n",
    "        bars = axes[2, 0].bar(bits, improvements, alpha=0.7, \n",
    "                             color=['green' if i > 0 else 'red' for i in improvements],\n",
    "                             edgecolor='black')\n",
    "        axes[2, 0].set_xlabel('Bit Width')\n",
    "        axes[2, 0].set_ylabel('Loss Improvement (%)')\n",
    "        axes[2, 0].set_title('Loss Improvement Within Each Bit-Width')\n",
    "        axes[2, 0].set_xticks(bits)\n",
    "        axes[2, 0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[2, 0].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, improvements):\n",
    "            height = bar.get_height()\n",
    "            axes[2, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{val:.1f}%', ha='center', \n",
    "                           va='bottom' if val > 0 else 'top', fontsize=9)\n",
    "    \n",
    "    # 6. Cumulative time spent at each bit-width\n",
    "    bit_counts = {}\n",
    "    for bit in unique_bits:\n",
    "        bit_counts[bit] = bit_counts.get(bit, 0) + 1\n",
    "    \n",
    "    if bit_counts:\n",
    "        bits = sorted(bit_counts.keys())\n",
    "        counts = [bit_counts[b] for b in bits]\n",
    "        percentages = [c / sum(counts) * 100 for c in counts]\n",
    "        \n",
    "        wedges, texts, autotexts = axes[2, 1].pie(percentages, \n",
    "                                                   labels=[f'{b}-bit' for b in bits],\n",
    "                                                   autopct='%1.1f%%',\n",
    "                                                   startangle=90,\n",
    "                                                   colors=plt.cm.viridis(np.linspace(0.3, 0.9, len(bits))))\n",
    "        axes[2, 1].set_title('Time Distribution Across Bit-Widths')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n📊 {name} - Bit-Width Switching Statistics:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total transitions: {len(transitions)}\")\n",
    "    if transitions:\n",
    "        print(f\"Average loss change at transition: {np.mean(transition_losses):.6f}\")\n",
    "        print(f\"Std dev of loss change: {np.std(transition_losses):.6f}\")\n",
    "        positive_transitions = sum(1 for t in transition_losses if t > 0)\n",
    "        print(f\"Transitions causing loss increase: {positive_transitions}/{len(transitions)} ({positive_transitions/len(transitions)*100:.1f}%)\")\n",
    "\n",
    "# Analyze switching patterns for each training type\n",
    "for name, stats in all_stats.items():\n",
    "    analyze_bit_switching_patterns(stats, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimal Bit-Width Determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_bitwidth(results):\n",
    "    \"\"\"Determine the optimal bit-width based on loss and efficiency metrics.\"\"\"\n",
    "    \n",
    "    for training_name, data in results.items():\n",
    "        print(f\"\\n🎯 {training_name} - Optimal Bit-Width Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        df_stats = data['stats']\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        df_stats['Efficiency'] = 1 / (df_stats['Mean Loss'] * df_stats['Bit Width'])\n",
    "        df_stats['Stability'] = 1 / (1 + df_stats['Std Dev'])\n",
    "        df_stats['Performance'] = 1 / df_stats['Mean Loss']\n",
    "        \n",
    "        # Normalize metrics to [0, 1]\n",
    "        for col in ['Efficiency', 'Stability', 'Performance']:\n",
    "            df_stats[f'{col}_Norm'] = (df_stats[col] - df_stats[col].min()) / (df_stats[col].max() - df_stats[col].min())\n",
    "        \n",
    "        # Calculate composite score (weighted average)\n",
    "        weights = {'Efficiency': 0.4, 'Stability': 0.3, 'Performance': 0.3}\n",
    "        df_stats['Composite_Score'] = (\n",
    "            weights['Efficiency'] * df_stats['Efficiency_Norm'] +\n",
    "            weights['Stability'] * df_stats['Stability_Norm'] +\n",
    "            weights['Performance'] * df_stats['Performance_Norm']\n",
    "        )\n",
    "        \n",
    "        # Sort by composite score\n",
    "        df_sorted = df_stats.sort_values('Composite_Score', ascending=False)\n",
    "        \n",
    "        print(\"\\n📈 Ranking by Composite Score:\")\n",
    "        display(df_sorted[['Bit Width', 'Mean Loss', 'Std Dev', 'Efficiency_Norm', \n",
    "                          'Stability_Norm', 'Performance_Norm', 'Composite_Score']].round(4))\n",
    "        \n",
    "        # Determine optimal bit-width\n",
    "        optimal_bit = df_sorted.iloc[0]['Bit Width']\n",
    "        print(f\"\\n✅ Optimal Bit-Width: {int(optimal_bit)}-bit\")\n",
    "        print(f\"   - Mean Loss: {df_sorted.iloc[0]['Mean Loss']:.4f}\")\n",
    "        print(f\"   - Stability (1/std): {df_sorted.iloc[0]['Stability']:.4f}\")\n",
    "        print(f\"   - Composite Score: {df_sorted.iloc[0]['Composite_Score']:.4f}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'{training_name} - Bit-Width Optimization Metrics', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Plot 1: Mean Loss vs Bit Width\n",
    "        axes[0].plot(df_stats['Bit Width'], df_stats['Mean Loss'], 'o-', markersize=8, linewidth=2)\n",
    "        axes[0].scatter(optimal_bit, df_sorted.iloc[0]['Mean Loss'], \n",
    "                       color='red', s=100, marker='*', zorder=5, label='Optimal')\n",
    "        axes[0].set_xlabel('Bit Width')\n",
    "        axes[0].set_ylabel('Mean Loss')\n",
    "        axes[0].set_title('Mean Loss vs Bit Width')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot 2: Normalized Metrics Comparison\n",
    "        x = np.arange(len(df_stats))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[1].bar(x - width, df_stats['Efficiency_Norm'], width, label='Efficiency', alpha=0.8)\n",
    "        axes[1].bar(x, df_stats['Stability_Norm'], width, label='Stability', alpha=0.8)\n",
    "        axes[1].bar(x + width, df_stats['Performance_Norm'], width, label='Performance', alpha=0.8)\n",
    "        \n",
    "        axes[1].set_xlabel('Bit Width')\n",
    "        axes[1].set_ylabel('Normalized Score')\n",
    "        axes[1].set_title('Normalized Metrics Comparison')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(df_stats['Bit Width'].astype(int))\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Plot 3: Composite Score\n",
    "        bars = axes[2].bar(df_stats['Bit Width'], df_stats['Composite_Score'], \n",
    "                          color='skyblue', edgecolor='black', linewidth=1.5)\n",
    "        \n",
    "        # Highlight optimal\n",
    "        optimal_idx = df_stats[df_stats['Bit Width'] == optimal_bit].index[0]\n",
    "        bars[optimal_idx].set_color('gold')\n",
    "        \n",
    "        axes[2].set_xlabel('Bit Width')\n",
    "        axes[2].set_ylabel('Composite Score')\n",
    "        axes[2].set_title('Composite Score (Higher is Better)')\n",
    "        axes[2].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, df_stats['Composite_Score']):\n",
    "            height = bar.get_height()\n",
    "            axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                        f'{score:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Determine optimal bit-width\n",
    "if results:\n",
    "    determine_optimal_bitwidth(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convergence Analysis per Bit-Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_convergence_per_bitwidth(results):\n",
    "    \"\"\"Analyze convergence characteristics for each bit-width.\"\"\"\n",
    "    \n",
    "    for training_name, data in results.items():\n",
    "        losses_dict = data['losses']\n",
    "        \n",
    "        print(f\"\\n📉 {training_name} - Convergence Analysis per Bit-Width\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(f'{training_name} - Convergence Characteristics', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        convergence_stats = []\n",
    "        \n",
    "        for bit_width in sorted(losses_dict.keys()):\n",
    "            losses = losses_dict[bit_width]\n",
    "            \n",
    "            if len(losses) > 10:\n",
    "                # Calculate convergence metrics\n",
    "                initial_loss = np.mean(losses[:5])\n",
    "                final_loss = np.mean(losses[-5:])\n",
    "                improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "                \n",
    "                # Estimate convergence rate (exponential decay fit)\n",
    "                x = np.arange(len(losses))\n",
    "                y = losses\n",
    "                \n",
    "                # Simple linear regression on log scale for exponential decay\n",
    "                try:\n",
    "                    # Avoid log of negative or zero values\n",
    "                    positive_losses = np.array(losses) - np.min(losses) + 0.001\n",
    "                    z = np.polyfit(x, np.log(positive_losses), 1)\n",
    "                    decay_rate = -z[0]\n",
    "                except:\n",
    "                    decay_rate = 0\n",
    "                \n",
    "                # Calculate stability (coefficient of variation of last 25%)\n",
    "                last_quarter = losses[-(len(losses)//4):]\n",
    "                stability = np.std(last_quarter) / np.mean(last_quarter) if np.mean(last_quarter) > 0 else np.inf\n",
    "                \n",
    "                convergence_stats.append({\n",
    "                    'Bit Width': bit_width,\n",
    "                    'Initial Loss': initial_loss,\n",
    "                    'Final Loss': final_loss,\n",
    "                    'Improvement (%)': improvement,\n",
    "                    'Decay Rate': decay_rate,\n",
    "                    'Stability (CV)': stability,\n",
    "                    'Samples': len(losses)\n",
    "                })\n",
    "                \n",
    "                # Plot loss trajectory with smoothing\n",
    "                axes[0, 0].plot(range(len(losses)), losses, alpha=0.3, label=f'{bit_width}-bit')\n",
    "                \n",
    "                # Add smoothed line\n",
    "                if len(losses) > 20:\n",
    "                    window = min(21, len(losses) // 5)\n",
    "                    if window % 2 == 0:\n",
    "                        window += 1\n",
    "                    smoothed = savgol_filter(losses, window, 3)\n",
    "                    axes[0, 0].plot(range(len(smoothed)), smoothed, linewidth=2, label=f'{bit_width}-bit (smooth)')\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Sample Index')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Loss Trajectories with Smoothing')\n",
    "        axes[0, 0].legend(loc='best', ncol=2)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        if convergence_stats:\n",
    "            df_conv = pd.DataFrame(convergence_stats)\n",
    "            \n",
    "            # Plot 2: Improvement percentage\n",
    "            axes[0, 1].bar(df_conv['Bit Width'], df_conv['Improvement (%)'], \n",
    "                          color='green', alpha=0.7, edgecolor='black')\n",
    "            axes[0, 1].set_xlabel('Bit Width')\n",
    "            axes[0, 1].set_ylabel('Improvement (%)')\n",
    "            axes[0, 1].set_title('Loss Improvement Percentage')\n",
    "            axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Plot 3: Decay rate\n",
    "            axes[1, 0].plot(df_conv['Bit Width'], df_conv['Decay Rate'], \n",
    "                           'o-', markersize=8, linewidth=2, color='blue')\n",
    "            axes[1, 0].set_xlabel('Bit Width')\n",
    "            axes[1, 0].set_ylabel('Decay Rate')\n",
    "            axes[1, 0].set_title('Convergence Speed (Decay Rate)')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 4: Final loss vs stability\n",
    "            scatter = axes[1, 1].scatter(df_conv['Final Loss'], df_conv['Stability (CV)'], \n",
    "                                        s=100, c=df_conv['Bit Width'], cmap='viridis', \n",
    "                                        edgecolors='black', linewidth=1.5)\n",
    "            axes[1, 1].set_xlabel('Final Loss')\n",
    "            axes[1, 1].set_ylabel('Stability (CV)')\n",
    "            axes[1, 1].set_title('Final Loss vs Stability Trade-off')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=axes[1, 1])\n",
    "            cbar.set_label('Bit Width')\n",
    "            \n",
    "            # Add annotations for each point\n",
    "            for idx, row in df_conv.iterrows():\n",
    "                axes[1, 1].annotate(f\"{int(row['Bit Width'])}b\", \n",
    "                                   (row['Final Loss'], row['Stability (CV)']),\n",
    "                                   xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Display convergence statistics table\n",
    "            print(\"\\n📊 Convergence Statistics:\")\n",
    "            display(df_conv.round(4))\n",
    "            \n",
    "            # Find best configurations\n",
    "            print(\"\\n🏆 Best Configurations:\")\n",
    "            print(f\"  - Fastest Convergence: {df_conv.loc[df_conv['Decay Rate'].idxmax(), 'Bit Width']:.0f}-bit\")\n",
    "            print(f\"  - Best Final Loss: {df_conv.loc[df_conv['Final Loss'].idxmin(), 'Bit Width']:.0f}-bit\")\n",
    "            print(f\"  - Most Stable: {df_conv.loc[df_conv['Stability (CV)'].idxmin(), 'Bit Width']:.0f}-bit\")\n",
    "            print(f\"  - Highest Improvement: {df_conv.loc[df_conv['Improvement (%)'].idxmax(), 'Bit Width']:.0f}-bit\")\n",
    "\n",
    "# Analyze convergence\n",
    "if results:\n",
    "    analyze_convergence_per_bitwidth(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_report(all_stats, results):\n",
    "    \"\"\"Generate a comprehensive summary report.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"📊 COMPREHENSIVE BIT-WIDTH ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for training_name in all_stats.keys():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📈 {training_name} Training Summary\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        stats = all_stats[training_name]\n",
    "        \n",
    "        # Basic statistics\n",
    "        if 'iteration_losses' in stats:\n",
    "            losses = stats['iteration_losses']\n",
    "            print(f\"\\n📊 Overall Statistics:\")\n",
    "            print(f\"  - Total iterations: {len(losses)}\")\n",
    "            print(f\"  - Final loss: {losses[-1]:.4f}\")\n",
    "            print(f\"  - Best loss: {min(losses):.4f}\")\n",
    "            print(f\"  - Average loss: {np.mean(losses):.4f}\")\n",
    "        \n",
    "        # Bit-width specific summary\n",
    "        if training_name in results:\n",
    "            df_stats = results[training_name]['stats']\n",
    "            \n",
    "            print(f\"\\n🔢 Bit-Width Performance Summary:\")\n",
    "            for _, row in df_stats.iterrows():\n",
    "                print(f\"  {int(row['Bit Width'])}-bit:\")\n",
    "                print(f\"    - Mean Loss: {row['Mean Loss']:.4f}\")\n",
    "                print(f\"    - Std Dev: {row['Std Dev']:.4f}\")\n",
    "                print(f\"    - Min Loss: {row['Min Loss']:.4f}\")\n",
    "                print(f\"    - Samples: {int(row['Samples'])}\")\n",
    "        \n",
    "        # Training insights\n",
    "        if 'bit_width_usage' in stats:\n",
    "            bit_widths = [b for b in stats['bit_width_usage'] if b is not None]\n",
    "            if bit_widths:\n",
    "                print(f\"\\n💡 Training Insights:\")\n",
    "                print(f\"  - Bit-widths used: {sorted(set(bit_widths))}\")\n",
    "                print(f\"  - Average bit-width: {np.mean(bit_widths):.2f}\")\n",
    "                print(f\"  - Most frequent bit-width: {max(set(bit_widths), key=bit_widths.count)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"💡 KEY RECOMMENDATIONS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(\"\"\"\n",
    "1. **Optimal Bit-Width Selection**:\n",
    "   - For best accuracy: Use 16-bit precision\n",
    "   - For best efficiency: Use 4-bit or 8-bit depending on accuracy requirements\n",
    "   - For balanced performance: 8-bit provides good trade-off\n",
    "\n",
    "2. **Training Strategy**:\n",
    "   - Cyclic precision training can help explore different bit-widths\n",
    "   - Start with higher precision and gradually reduce for better stability\n",
    "   - Monitor loss spikes during bit-width transitions\n",
    "\n",
    "3. **Convergence Optimization**:\n",
    "   - Lower bit-widths may require more iterations to converge\n",
    "   - Consider adaptive learning rates based on current bit-width\n",
    "   - Use warm-up periods after bit-width transitions\n",
    "\n",
    "4. **Production Deployment**:\n",
    "   - Test the selected bit-width on validation data\n",
    "   - Consider hardware constraints when selecting bit-width\n",
    "   - Profile memory and computation savings for each bit-width\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✅ Analysis Complete!\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "# Generate final summary\n",
    "if all_stats:\n",
    "    generate_summary_report(all_stats, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}