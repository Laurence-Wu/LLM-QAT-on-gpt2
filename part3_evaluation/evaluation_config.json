{
  "device": "cuda",
  "calibration": {
    "dataset": "wikitext",
    "dataset_config": "wikitext-2-raw-v1",
    "split": "validation",
    "num_samples": 100,
    "batch_size": 4,
    "max_length": 128,
    "warm_up_batches": 10,
    "truncation": true,
    "padding": "max_length"
  },
  "zero_shot": {
    "max_samples": 500,
    "max_errors": 10,
    "show_first_n_errors": 3,
    "datasets": {
      "BoolQ": {
        "dataset_name": "boolq",
        "split": "validation[:1000]"
      },
      "HellaSwag": {
        "dataset_name": "hellaswag",
        "split": "validation[:1000]"
      },
      "WinoGrande": {
        "dataset_name": "winogrande",
        "config": "winogrande_m",
        "split": "validation[:1000]"
      },
      "ARC-e": {
        "dataset_name": "ai2_arc",
        "config": "ARC-Easy",
        "split": "validation[:1000]"
      },
      "ARC-c": {
        "dataset_name": "ai2_arc",
        "config": "ARC-Challenge",
        "split": "validation[:1000]"
      },
      "OBQA": {
        "dataset_name": "openbookqa",
        "config": "main",
        "split": "validation[:1000]"
      }
    },
    "generation": {
      "max_length": 10,
      "temperature": 0.1,
      "do_sample": false
    },
    "prompt_truncation": {
      "limited_context_threshold": 256,
      "passage_limit": 200,
      "question_limit": 100,
      "context_limit": 150,
      "choice_limit": 60,
      "endings_limit": 50
    }
  },
  "few_shot": {
    "num_shots": 5,
    "max_samples": 500,
    "max_errors": 10,
    "show_first_n_errors": 3,
    "datasets": {
      "MMLU": {
        "dataset_name": "cais/mmlu",
        "config": "all",
        "split": "test[:1000]"
      },
      "TriviaQA": {
        "dataset_name": "trivia_qa",
        "config": "rc.nocontext",
        "split": "validation[:1000]"
      }
    },
    "generation": {
      "max_length": 20,
      "temperature": 0.1,
      "do_sample": false
    },
    "mmlu_categories": {
      "Humanities": ["history", "philosophy", "law"],
      "STEM": ["physics", "chemistry", "biology", "computer_science", "math", "engineering"],
      "Social Sciences": ["politics", "sociology", "psychology", "economics"],
      "Other": ["other", "business", "health"]
    }
  },
  "perplexity": {
    "stride": 128,
    "max_length": 256,
    "max_samples": 100,
    "batch_size": 1,
    "datasets": {
      "WikiText2": {
        "dataset_name": "wikitext",
        "config": "wikitext-2-raw-v1",
        "split": "test"
      },
      "C4": {
        "dataset_name": "allenai/c4",
        "config": "en",
        "split": "validation[:1000]",
        "streaming": true
      }
    }
  },
  "output": {
    "directory": "part3_evaluation/results",
    "save_format": "json",
    "results_filename": "llm_qat_results.json"
  },
  "model": {
    "default_precision": 32,
    "require_cuda": true,
    "bit_to_config_mapping": {
      "6": "INT6",
      "8": "INT8",
      "16": "FP16",
      "32": "FP32"
    }
  }
}